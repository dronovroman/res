import logging
import os
import torch
from transformers import Trainer, TrainingArguments, TrainerCallback, TrainerState, TrainerControl, PreTrainedModel
from typing import Dict, Optional

# Assuming your model, tokenizer, datasets are defined somewhere above
# model = ...
# tokenizer = ...
# train_dataset = ...
# eval_dataset = ... # Optional
# compute_metrics = ... # Optional function for evaluation


# --- Custom Trainer to Calculate Grad Norm ---
class TrainerWithGradNorm(Trainer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # Initialize a variable to store the grad norm
        self.state.last_grad_norm = None # Use TrainerState to store temporary values

    def training_step(self, model: torch.nn.Module, inputs: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        Perform a training step on a batch of inputs.

        Subclass and override to compute and store grad norm before optimizer.step()
        and zero_grad().
        """
        model.train()
        inputs = self._prepare_inputs(inputs)

        # --- Standard Forward Pass ---
        with self.compute_loss_context_manager():
            loss = self.compute_loss(model, inputs)

        if self.args.n_gpu > 1:
            loss = loss.mean()  # mean() to average on multi-gpu parallel training

        # --- Backward Pass ---
        # self.accelerator.backward(loss) handles gradient accumulation and scaling
        self.accelerator.backward(loss)

        # --- Calculate Grad Norm ---
        # This happens *after* backward() and *before* optimizer.step() / zero_grad()
        # Important: Only calculate the norm when an optimizer step is actually going to happen
        #            (i.e., not in the middle of gradient accumulation)
        is_optimizer_step = (self.state.global_step + 1) % self.args.gradient_accumulation_steps == 0 \
                           or (self.state.num_training_steps == self.state.global_step + 1) # Last step

        grad_norm = None # Initialize to None for steps without optimizer update
        if self.accelerator.sync_gradients and is_optimizer_step:
            # Need to unscale gradients before clipping/norm calculation if using AMP
            self.accelerator.unscale_gradients()

            parameters_with_grad = [p for p in model.parameters() if p.grad is not None]
            if parameters_with_grad: # Ensure there are grads
                # Calculate the norm *before* clipping if you want unclipped norm
                # Or calculate *after* clipping if you want the potentially clipped norm
                # self.accelerator.clip_grad_norm_ handles clipping

                # Calculate norm (using same logic as callback, but at the right time)
                device = parameters_with_grad[0].grad.device
                try:
                    # Use float32 for stability
                    stacked_norms = torch.stack([
                        torch.norm(p.grad.detach().to(torch.float32), 2).to(device)
                        for p in parameters_with_grad
                    ])
                    grad_norm = torch.norm(stacked_norms, 2).item()
                except Exception as e:
                    logging.warning(f"Could not compute grad norm during training step: {e}")
                    grad_norm = None # Set to None on error

            # Apply gradient clipping (Trainer/Accelerator usually does this before optimizer step)
            # Make sure your calculation doesn't interfere with the Trainer's internal clipping
            if self.args.max_grad_norm is not None and self.args.max_grad_norm > 0:
                 self.accelerator.clip_grad_norm_(model.parameters(), self.args.max_grad_norm)

        # Store the calculated norm in the state
        self.state.last_grad_norm = grad_norm

        # --- Optimizer Step & Zero Grad (handled by Trainer/Accelerator) ---
        # Note: The original `Trainer.training_step` doesn't explicitly call optimizer.step()
        #       or zero_grad() here. These are handled within the main train loop
        #       calling this `training_step`. The key is that we calculated the norm
        #       *before* those actions happen on the gradients computed in *this* step.

        return loss.detach() / self.args.gradient_accumulation_steps


# --- Updated Custom Callback (Simplified) ---
class FileLoggingCallback(TrainerCallback):
    """
    Logs metrics including grad norm (retrieved from Trainer state).
    """
    def __init__(self, log_file_path="training_log.log"):
        super().__init__()
        self.log_file_path = log_file_path
        log_dir = os.path.dirname(log_file_path)
        if log_dir and not os.path.exists(log_dir):
             os.makedirs(log_dir, exist_ok=True)
        with open(self.log_file_path, "w") as f:
            f.write("Step\tEpoch\tLoss\tLearningRate\tGradNorm\n") # Added GradNorm header

    # No need for model reference here anymore
    # No need for _get_grad_norm here anymore

    def on_log(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, logs: Optional[Dict[str, float]] = None, **kwargs):
        if state.is_world_process_zero and logs is not None:
            step = state.global_step
            # Prefer epoch from logs if available (more precise), else use state
            epoch = logs.get("epoch", state.epoch)

            log_line_parts = [f"{step}", f"{epoch:.2f}"]

            # --- Log Training Loss & LR (if present) ---
            loss_str = f"{logs.get('loss', 'N/A'):.4f}" if 'loss' in logs else "N/A"
            lr_str = f"{logs.get('learning_rate', 'N/A'):.2e}" if 'learning_rate' in logs else "N/A"

            if 'loss' in logs: # Typically indicates a training log
                log_line_parts.extend([loss_str, lr_str])

                # --- Retrieve and Log Grad Norm from state ---
                # Get the grad norm stored by the custom Trainer
                grad_norm = getattr(state, 'last_grad_norm', None) # Safely get the attribute
                grad_norm_str = f"{grad_norm:.4f}" if grad_norm is not None else "N/A"
                log_line_parts.append(grad_norm_str)

                # Optionally log other metrics
                steps_per_sec = logs.get('train_steps_per_second', None)
                if steps_per_sec:
                     log_line_parts.append(f"{steps_per_sec:.2f} steps/s")

            # --- Log Evaluation Metrics (if present) ---
            elif 'eval_loss' in logs:
                eval_loss_str = f"{logs.get('eval_loss', 'N/A'):.4f}"
                log_line_parts.extend([f"Eval Loss: {eval_loss_str}"])
                for key, value in logs.items():
                    if key.startswith("eval_") and key != "eval_loss":
                         log_line_parts.append(f"{key}: {value:.4f}")

            # Append to the log file
            try:
                with open(self.log_file_path, "a") as f:
                    f.write("\t".join(log_line_parts) + "\n")
            except Exception as e:
                logging.warning(f"Could not write to log file {self.log_file_path}: {e}")


# --- Configure Training Arguments ---
training_args = TrainingArguments(
    output_dir='./results_gradnorm',         # Changed output dir
    num_train_epochs=1,
    per_device_train_batch_size=4,
    logging_dir='./logs_gradnorm',           # Changed log dir
    logging_steps=10,                       # Log frequently to see results
    report_to="none",                       # Disable other reporting for clarity
    max_grad_norm=1.0,                      # Keep gradient clipping enabled
    gradient_accumulation_steps=1,          # Simplest case first
    # evaluation_strategy="steps",          # Optional: Add if you want eval metrics logged
    # eval_steps=100,                       # Optional: Frequency of evaluation
    # ... other arguments
)

# --- Instantiate the Callback and CUSTOM Trainer ---
file_logger = FileLoggingCallback(
    log_file_path="my_training_run_with_gradnorm_fixed.log"
)

# Use the custom Trainer subclass
trainer = TrainerWithGradNorm( # <<< Use the subclass
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    # eval_dataset=eval_dataset,
    # compute_metrics=compute_metrics,
    tokenizer=tokenizer,
    callbacks=[file_logger] # Pass the updated callback
)

# --- Start Training ---
trainer.train()
